{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Values\n",
    "\n",
    "TensorFlow里面的数据单位就是Tensor(张量). 一个张量由任意维的数组值组成. rank表示张量的维数, shape是一个整数元祖(tuple), 表示每个维度上数组的长度. 例如\n",
    "\n",
    "```python\n",
    "3. # a rank 0 tensor; a scalar with shape [],\n",
    "[1., 2., 3.] # a rank 1 tensor; a vector with shape [3]\n",
    "[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]\n",
    "[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]\n",
    "```\n",
    "\n",
    "## TensorFlow Core\n",
    "\n",
    "TensorFlow Core程序由两个独立的部分组成\n",
    "\n",
    "1. 构建计算图(tf.Graph).\n",
    "2. 运行计算图(tf.Session).\n",
    "\n",
    "### Graph\n",
    "\n",
    "计算图是一系列TensorFlow操作排列成的图. 该图由两种类型的对象组成. \n",
    "- Operations(或\"ops\")：图的节点. 描述消耗和产生张量的需要的计算.\n",
    "- Tensor: 图中的边. 这些代表将通过图流动的值. 大多数TensorFlow函数返回`tf.Tensors`.\n",
    "\n",
    "> 重要提示: tf.Tensors没有值, 它们只是计算图中元素的句柄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # any {'0', '1', '2'} tensorflow suppress warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = tf.constant(3.0, dtype=tf.float32)\n",
    "b = tf.constant(4.0) # also tf.float32 implicitly\n",
    "total = a + b\n",
    "print(a)\n",
    "print(b)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意, 打印张量不会像您所期望的那样输出值3.0,4.0和7.0. 以上仅构建计算图. 这些tf.Tensor对象仅代表这些操作将要运行的结果.\n",
    "\n",
    "计算图中的每个操作都有一个唯一的名称. 这个名字与在Python中分配给对象的名称无关. 张量以产生它们的操作命名, 后跟输出索引, 如上面的`add：0`所示.\n",
    "\n",
    "### TensorBoard\n",
    "\n",
    "TensorFlow提供了一个名为TensorBoard的实用程序. TensorBoard的许多功能之一是可视化计算图. 您可以使用几个简单的命令轻松完成此操作.\n",
    "\n",
    "1. 首先将计算图保存为TensorBoard摘要文件，如下所示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这将在当前目录中生成一个事件文件，其中包含以下形式的名称\n",
    "\n",
    "```python\n",
    "events.out.tfevents.{timestamp}.{hostname}\n",
    "```\n",
    "\n",
    "现在, 在一个新的终端中, 使用以下shell命令启动TensorBoard\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir .\n",
    "```\n",
    "\n",
    "### Session\n",
    "\n",
    "为了evaluate张量，实例化一个tf.Session对象，非正式地称为会话。会话封装了TensorFlow运行时的状态，并运行TensorFlow操作。如果`tf.Graph`像`.py`文件，则`tf.Session`就像`python`可执行文件。\n",
    "\n",
    "\n",
    "下面的代码创建一个tf.Session对象，然后调用它的run方法来评估我们在上面创建的`total`张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以传递多个Tensor, `run`方法可以流畅地处理元组或字典的任何组合，如下例所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run({'ab': (a, b), 'total': total}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在调用`tf.Session.run`期间，任何tf.Tensor只有一个值。例如，下面的代码调用`tf.random_uniform`来产生一个tf.Tensor，它随机生成一个3元素向量（值范围为`[0,1)`）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = tf.random_uniform(shape=(3,))\n",
    "out1 = vec + 1\n",
    "out2 = vec + 2\n",
    "print(sess.run(vec))\n",
    "print(sess.run(vec))\n",
    "print(sess.run((out1, out2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**结果显示每次调用运行时都有一个不同的随机值**，但在单次运行期间（`out1`和`out2`得到的是相同的`vec`随机输入值）.\n",
    "\n",
    "一些TensorFlow函数返回`tf.Operations`而不是`tf.Tensors`。在Operation上调用run的结果是None。运行一个操作是为了cause a side-effect，而不是检索一个值。这方面的例子包括initialization和稍后演示的training ops。\n",
    "\n",
    "### Feeding\n",
    "\n",
    "就目前来看，这张图并不特别有趣，因为它总是会产生一个常量的结果。图形可以被参数化以接受外部输入, 这种称为占位符的。占位符是稍后提供值，如函数参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面的三行有点像我们定义了一个有两个输入参数（x和y）的函数。我们可以通过使用run方法的feed_dict参数将多个输入的具体值提供给占位符来评估此图形。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(z, feed_dict={x: 3, y: 4.5}))\n",
    "print(sess.run(z, feed_dict={x: [1, 3], y: [2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 另请注意，feed_dict参数可用于覆盖graph中的任何张量。占位符和其他tf.Tensors之间唯一的区别是，如果没有给它们赋值，占位符会抛出错误。\n",
    "\n",
    "### Datasets\n",
    "\n",
    "占位符适用于简单的实验，但数据集是将数据流式传输到模型的首选方法.\n",
    "\n",
    "要从Dataset中获取可运行的tf.Tensor，必须先将其转换为`tf.data.Iterator`，然后调用Iterator的`get_next`方法。\n",
    "\n",
    "创建Iterator的最简单方法是使用`make_one_shot_iterator`方法。例如，在下面的代码中，`next_item`张量将在每次运行调用中从my_data数组返回一行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = [\n",
    "    [0, 1,],\n",
    "    [2, 3,],\n",
    "    [4, 5,],\n",
    "    [6, 7,],\n",
    "]\n",
    "slices = tf.data.Dataset.from_tensor_slices(my_data)\n",
    "next_item = slices.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到达数据流的末尾会导致数据集抛出OutOfRangeError。例如，下面的代码读取next_item，直到没有更多数据要读取为止："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "  try:\n",
    "    print(sess.run(next_item))\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "具有相同输入情况下, 可训练的模型必须修改graph中的值(参数)以获得新输出。Layers是将可训练参数添加到图形的首选方法。\n",
    "\n",
    "Layers将变量和作用于它们的操作打包在一起。例如， densely-connected layer对每个输出的所有输入执行加权求和并应用可选激活函数。连接weights和biases由图层对象管理。\n",
    "\n",
    "### Creating Layers\n",
    "\n",
    "以下代码创建一个密集层，它接受一批输入向量，并为每个输出向量生成一个输出值。要将图层应用于输入，可以将该图层看成函数。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "linear_model = tf.layers.Dense(units=1)\n",
    "y = linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图层检查其输入以确定其内部变量的大小。因此，在这里我们必须设置x占位符的形状，以便图层可以构建正确大小的权重矩阵.\n",
    "\n",
    "既然我们已经定义了输出y的计算，那么在我们运行计算之前，还有一个细节需要处理。\n",
    "\n",
    "### Initializing Layers\n",
    "\n",
    "该图层包含了必须在可以使用之前进行初始化的变量。尽管可以单独初始化变量，但您可以轻松地初始化TensorFlow图中的所有变量，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 重要提示：调用`tf.global_variables_initializer`仅创建并返回TensorFlow操作的句柄。当我们用`tf.Session.run`运行它时，该操作将初始化所有全局变量。\n",
    "\n",
    "另请注意，此`global_variables_initializer`仅在初始化程序创建时初始化图中存在的变量。所以初始化应该是图构建过程中添加的最后一件事情之一。\n",
    "\n",
    "### Executing Layers\n",
    "\n",
    "现在该图层已初始化，我们可以像处理其他任何张量一样评估linear_model的输出张量。例如，下面的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(y, {x: [[1, 2, 3],[4, 5, 6]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Function shortcuts\n",
    "\n",
    "对于每个图层类（如tf.layers.Dense），TensorFlow还提供了一个快捷方式功能（如tf.layers.dense）。唯一的区别是shortcut版本在一次调用中创建并运行图层。例如，以下代码等同于较早版本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.layers.dense(x, units=1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run(y, {x: [[1, 2, 3], [4, 5, 6]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然方便，但此方法不允许访问tf.layers.Layer对象。这使内省和调试更加困难，并且不能重用layers。\n",
    "\n",
    "## Feature columns\n",
    "\n",
    "实验特征列的最简单方法是使用`tf.feature_column.input_layer`函数。此函数只接受dense column为输入，因此要查看categorical column的结果，必须将其包含在`tf.feature_column.indicator_column`中。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    'sales' : [[5], [10], [8], [9]],\n",
    "    'department': ['sports', 'sports', 'gardening', 'gardening']}\n",
    "\n",
    "department_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'department', ['sports', 'gardening'])\n",
    "department_column = tf.feature_column.indicator_column(department_column)\n",
    "\n",
    "columns = [\n",
    "    tf.feature_column.numeric_column('sales'),\n",
    "    department_column\n",
    "]\n",
    "\n",
    "inputs = tf.feature_column.input_layer(features, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行`inputs`张量将把`feature`解析为一批向量.\n",
    "\n",
    "特征列可以具有内部状态，比如图层，所以它们经常需要被初始化。Categorical columns在内部使用lookup表，这些需要单独的初始化操作`tf.tables_initializer`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_init = tf.global_variables_initializer()\n",
    "table_init = tf.tables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run((var_init, table_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦内部状态被初始化，你可以像任何其他tf.Tensor一样运行`inputs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这显示了feature columns如何打包input vectors，其中前两个索引是one-hot \"department\" ，第三个是“sales”。\n",
    "\n",
    "\n",
    "## Training\n",
    "\n",
    "现在您已熟悉TensorFlow核心的基础知识，我们手动培训一个小型回归模型。\n",
    "\n",
    "### Define the Data\n",
    "\n",
    "首先让我们定义一些输入x和每个输入的预期输出y_true："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)\n",
    "y_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model\n",
    "\n",
    "接下来，建立一个简单的线性模型，带有1个输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = tf.layers.Dense(units=1)\n",
    "\n",
    "y_pred = linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以按如下方式计算预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该模型尚未训练，所以四个“预测”值不是很好。\n",
    "\n",
    "### Loss\n",
    "\n",
    "要优化模型，首先需要定义损失。我们将使用均方误差，这是回归问题的标准损失。\n",
    "\n",
    "虽然您可以在较低级别的数学运算中手动执行此操作，但tf.losses模块提供了一组常用损失函数。您可以使用它来计算均方误差，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\n",
    "\n",
    "print(sess.run(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "TensorFlow提供了执行标准优化算法的optimizer。这些实现为`tf.train.Optimizer`的子类。他们逐渐改变graph中每个变量以最小化损失。最简单的优化算法是梯度下降法，由`tf.train.GradientDescentOptimizer`实现。它根据相对于该变量的损失导数的大小来修改每个变量。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该代码构建了优化所需的所有graph组件，并返回一个训练操作。运行时，训练操作将更新图形中的变量。您可以按如下方式运行它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  _, loss_value = sess.run((train, loss))\n",
    "  print(loss_value)\n",
    "print(sess.run(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
